{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: RNN, LSTM, Attention, and Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author: Chang Chen (TA) and Sungjin Ahn (Instructor)**\n",
    "\n",
    "In this assignment, we will implement neural machine translation (NMT) models using RNNs, LSTMs, attention mechanisms, and Transformes. The assignmenet contains five modules\n",
    "1. Dataset preparation (Guide 1)\n",
    "1. Baseline seq2seq RNN (Guide 2)\n",
    "1. LSTM seq2seq (Task 1)\n",
    "1. Seq2seq with Attention (Task 2)\n",
    "1. Transformer (Task 3)\n",
    "\n",
    "First two modules are for guidance and the last three modules are tasks for you to implement. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Guide: Datasets\n",
    "\n",
    "In this section, we provide the dataset preparation guidance. You do not need to implement any code in this section.\n",
    "\n",
    "### 1.1 Data Download and Raw Text Preprocessing\n",
    "\n",
    "We will use the English-to-French dataset. As shown below, the dataset contains multiple lines each of which has a English sentence and its Frence translation separated by a TAB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_if_not_exist(file_name):\n",
    "  \n",
    "  if not os.path.exists(file_name):\n",
    "    import urllib.request\n",
    "    DATA_URL = 'https://download.pytorch.org/tutorial/data.zip'\n",
    "\n",
    "    file_name, _ = urllib.request.urlretrieve(DATA_URL, './data.zip')\n",
    "    \n",
    "  return file_name\n",
    "\n",
    "def read_raw(file_name):\n",
    "  file_name = download_if_not_exist(file_name)\n",
    "  \n",
    "  with zipfile.ZipFile(file_name, 'r') as fzip:\n",
    "    raw_text = fzip.read(file_name.split('.')[-2][1:] + '/eng-fra.txt').decode('utf-8')\n",
    "  return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Wow!\tÇa alors !\n",
      "Fire!\tAu feu !\n",
      "Help!\tÀ l'aide !\n",
      "Jump.\tSaute.\n",
      "Stop!\tÇa suffit !\n",
      "Stop!\tStop !\n",
      "Stop!\tArrête-toi !\n",
      "Wait!\tAttends !\n",
      "Wait!\tAttendez !\n",
      "I see.\tJe comprends.\n"
     ]
    }
   ],
   "source": [
    "raw_text = read_raw('./data.zip')\n",
    "print(raw_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For raw text preprocessing, we need to replace special symbols with spaces and convert all characters into the lower cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw(text):\n",
    "  text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "  out = ''\n",
    "  for i, char in enumerate(text.lower()):\n",
    "    if char in (',', '!', '.') and i > 0 and text[i-1] != ' ':\n",
    "      out += ' '\n",
    "    out += char\n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further split the source-target pairs into the source list and the target list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_source_target(text, max_len):\n",
    "  source, target = [], []\n",
    "  for i, line in enumerate(text.split('\\n')):\n",
    "    if i > 5000: # we only use 20000 pairs of translation\n",
    "      break\n",
    "    parts = line.split('\\t')\n",
    "    if len(parts) == 2:\n",
    "      src_tokens = parts[0].split(' ')\n",
    "      tgt_tokens = parts[1].split(' ')\n",
    "      if (len(src_tokens) <= max_len) and (len(tgt_tokens) <= max_len):\n",
    "        source.append(src_tokens)\n",
    "        target.append(tgt_tokens)\n",
    "  return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(raw_text, max_len=10000):\n",
    "  text = preprocess_raw(raw_text)\n",
    "  source, target = split_source_target(text, max_len)\n",
    "  return source, target\n",
    "\n",
    "source, target = prepare_data(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the whole dataset takes too much memory, and it is also hard to train with a large vocabulary. Thus, we will filter out some words by looking at the statistical properties of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def len_dis(text):\n",
    "  lens = [len(line) for line in text]\n",
    "  len_counter = collections.Counter(lens)\n",
    "\n",
    "  lens = np.array(list(len_counter.keys()))\n",
    "  sort_idx = np.argsort(lens)\n",
    "  lens_sort = lens[sort_idx]\n",
    "  len_counts = np.array(list(len_counter.values()))\n",
    "  len_counts_sort = len_counts[sort_idx]\n",
    "  p = np.cumsum(len_counts_sort) / len_counts_sort.sum()\n",
    "  return p, lens_sort\n",
    "  \n",
    "src_p, src_lens_sort = len_dis(source)\n",
    "tgt_p, tgt_lens_sort = len_dis(target)\n",
    "plt.plot(src_lens_sort, src_p, 'r-', label='eng')\n",
    "plt.plot(tgt_lens_sort, tgt_p, 'g-', label='fra')\n",
    "plt.title('Cumulative Distribution of Sentence Length')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plots, we can see that more than 90% of the sentences have a length of less than 8. Thus, we can filter out sentences of length greater than 8. We also filter out words that occur less than 5 times in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-param\n",
    "MAX_LEN = 8\n",
    "MIN_FREQ = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Build Vocabulary\n",
    "\n",
    "Each word needs a unique index, and the words that have been filtered out need a special token to represent them. The following class Vocab is used to build the vocabulary. Some basic helper functions or dictionaries are also provided:\n",
    "- Dictionary word2index: Convert word string into index: \n",
    "- Dictionary index2word: Convert index into word string\n",
    "- helper function _build_vocab(): Build dictionaries for converting from words to indices and vice versa\n",
    "- Word Counter, num_word: Record the total number of unique tokens in the vocabulary \n",
    "    \n",
    "There are 4 special tokens added in the vocabulary:\n",
    "- 'pad': padding token. Sentences shorter than MAX_LEN is padded by this symbol to make the length to MAX_LEN\n",
    "- 'bos': beginning of sentence. This indicates the beginning of a sentence\n",
    "- 'eos': end of sentenct. This indicates the end of a sentence\n",
    "- 'unk': unknown word. This represents words that have been filtered out (words that are not in the vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "  def __init__(self, name, tokens, min_freq):\n",
    "    self.name = name\n",
    "    self.index2word = {\n",
    "      0: 'pad',\n",
    "      1: 'bos',\n",
    "      2: 'eos',\n",
    "      3: 'unk'\n",
    "    }\n",
    "    self.word2index = {v: k for k, v in self.index2word.items()}\n",
    "    self.num_word = 4\n",
    "    token_freq = collections.Counter(tokens)\n",
    "    tokens = [token for token in tokens if token_freq[token] >= MIN_FREQ]\n",
    "    self._build_vocab(tokens)\n",
    "    \n",
    "  def _build_vocab(self, tokens):\n",
    "    for token in tokens:\n",
    "      if token not in self.word2index:\n",
    "        self.word2index[token] = self.num_word\n",
    "        self.index2word[self.num_word] = token\n",
    "        self.num_word += 1\n",
    "        \n",
    "  def __getitem__(self, tokens):\n",
    "    if not isinstance(tokens, (list, tuple)):\n",
    "      return self.word2index.get(tokens, self.word2index['unk'])\n",
    "    else:\n",
    "      return [self.__getitem__(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Build Dataset\n",
    "\n",
    "The dataset pipeline involves the following steps:\n",
    "- For target language, every sentence will be 'sandwiched' with the 'bos' token and the 'eos' token.\n",
    "- Every sentence that has a length less than MAX_LEN will be padded to the MAX_LEN with the *padding_token*.\n",
    "- The dataset should return the converted tensor and the corresponding valid length before padding.\n",
    "- We use the Pytorch *DataLoader* API to build the dataset generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of source language: 433\n",
      "Vocabulary size of target language: 420\n",
      "Total number of sentence pairs: 4990\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(name, tokens, min_freq):\n",
    "  tokens = [token for line in tokens for token in line]\n",
    "  return Vocab(name, tokens, min_freq)\n",
    "\n",
    "def build_vocabs(lang_src, lang_tgt, src_text, tgt_text):\n",
    "  vocab_src = build_vocab(lang_src, src_text, MIN_FREQ)\n",
    "  vocab_tgt = build_vocab(lang_tgt, tgt_text, MIN_FREQ)\n",
    "  return vocab_src, vocab_tgt\n",
    "\n",
    "def pad(line, padding_token):\n",
    "  return line + [padding_token] * (MAX_LEN + 2 - len(line))\n",
    "\n",
    "def build_tensor(text, lang, is_source):\n",
    "  lines = [lang[line] for line in text]\n",
    "  if not is_source:\n",
    "    lines = [[lang['bos']] + line + [lang['eos']] for line in lines]\n",
    "  array = torch.tensor([pad(line, lang['pad']) for line in lines])\n",
    "  valid_len = (array != lang['pad']).sum(1)\n",
    "  return array, valid_len\n",
    "\n",
    "def load_data_nmt(batch_size=2):\n",
    "  lang_eng, lang_fra = build_vocabs('eng', 'fra', source, target)\n",
    "  src_array, src_valid_len = build_tensor(source, lang_eng, True)\n",
    "  tgt_array, tgt_valid_len = build_tensor(target, lang_fra, False)\n",
    "  train_data = torch.utils.data.TensorDataset(\n",
    "    src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "  train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "  return lang_eng, lang_fra, train_iter\n",
    "\n",
    "\n",
    "source, target = prepare_data(raw_text, max_len=MAX_LEN)\n",
    "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size=2)\n",
    "print('Vocabulary size of source language: {}'.format(vocab_eng.num_word))\n",
    "print('Vocabulary size of target language: {}'.format(vocab_fra.num_word))\n",
    "print('Total number of sentence pairs: {}'.format(len(source)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Guidance: Sequence to Sequence with RNN (baseline)\n",
    "\n",
    "In this section, we provide the implementation of the seq2seq baseline model. You do not need to implement any code in this section. The following figure highlights the architecture of the seq2seq model. An encoder RNN encodes the input sequence into its hidden state, and passes the last hidden state to the decoder RNN. The decoder generates the target sequence.\n",
    "\n",
    "Implementation Details:\n",
    "\n",
    "- Embedding: We have represented each word with an integer or one-hot vector. We need an embedding layer to map an input word to its embedding vector.\n",
    "- Encoder: A vanilla RNN is used to encode a source sequence. The final hidden state is returned as output and passed to the decoder RNN.\n",
    "- Decoder: Another vanilla RNN is implemented to generate the target sequence. The hidden state is initialized with the last hidden state from the encoder.\n",
    "- Encoder-Decoder: The class NMTRNN is built by combining the encoder and the decoder, and yields the loss and predictions.\n",
    "- Loss: We have padded all sentences so that they have the same MAX_LEN. Thus, when we compute the loss, the loss from those padding_tokens should be masked out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./img/seq2seq.svg\" width=\"600\"/>\n",
    "</div>\n",
    "Image source: https://github.com/dsgiitr/d2l-pytorch/blob/master/img/seq2seq.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "    super(Encoder, self).__init__()\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "      vocab_size: int, the number of words in the vocabulary\n",
    "      embedding_dim: int, dimension of the word embedding\n",
    "      hidden_size: int, dimension of the hidden state of vanilla RNN\n",
    "    \"\"\"\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim) # embedding layer\n",
    "    self.enc = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "    self.hidden_size = hidden_size\n",
    "    \n",
    "  def forward(self, sources, valid_len):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      source: tensor of size (N, T), where N is the batch size, T is the length of the sequence(s)\n",
    "      valid_len: tensor of size (N,), indicating the valid length of sequence(s) (the length before padding)\n",
    "    \"\"\"\n",
    "    word_embedded = self.embedding(sources)\n",
    "    N = word_embedded.shape[0]\n",
    "    \n",
    "    h = sources.new_zeros(1, N, self.hidden_size).float() # initialize hidden state with zeros\n",
    "    \n",
    "    o, h = self.enc(word_embedded, h)\n",
    "    \n",
    "    return o[np.arange(N), valid_len] # return the hidden state of the valid last time step\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "    super(Decoder, self).__init__()\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "      vocab_size: int, the number of words in the vocabulary\n",
    "      embedding_dim: int, dimension of the word embedding\n",
    "      hidden_size: int, dimension of the hidden state of vanilla RNN\n",
    "    \"\"\"\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.enc = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "    self.output_emb = nn.Linear(hidden_size, vocab_size)\n",
    "    self.hidden_size = hidden_size\n",
    "    \n",
    "  def forward(self, h, target):\n",
    "    word_embedded = self.embedding(target)\n",
    "    N, T = word_embedded.shape[:2]\n",
    "    \n",
    "    o, h = self.enc(word_embedded, h.view(1,N,self.hidden_size))\n",
    "    pred = self.output_emb(o)\n",
    "    return pred, h\n",
    "\n",
    "class NMTRNN(nn.Module):\n",
    "  def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size):\n",
    "    super(NMTRNN, self).__init__()\n",
    "    self.enc = Encoder(src_vocab_size, embedding_dim, hidden_size)\n",
    "    self.dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size)\n",
    "    \n",
    "  def forward(self, src, src_len, tgt, tgt_len):\n",
    "    h = self.enc(src, src_len)\n",
    "    T = tgt.shape[1]\n",
    "    \n",
    "    if self.training:\n",
    "      pred, _ = self.dec(h, tgt)\n",
    "      loss = 0\n",
    "      for t in range(T-1):\n",
    "        # target sequence should shift by one time-step, because we are predicting the next word\n",
    "        loss = loss + F.nll_loss(F.log_softmax(pred[:, t]), tgt[:, t+1], ignore_index=0)\n",
    "\n",
    "      return loss, pred.argmax(dim=-1)\n",
    "    else:\n",
    "      \"\"\"\n",
    "      When predicting a sequence given the 'bos' token, the input for the next step is the predicted\n",
    "      token from the previous time step.\n",
    "      \"\"\"\n",
    "      inputs = tgt[:, :1]\n",
    "      preds = []\n",
    "      for t in range(MAX_LEN+1): # plus the 'eos' token\n",
    "        pred, h = self.dec(h, inputs)\n",
    "        preds.append(pred)\n",
    "        inputs = pred.argmax(dim=-1)\n",
    "        \n",
    "      pred = torch.cat(preds, dim=1).argmax(dim=-1)\n",
    "      return pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 / 7800\tLoss:\t49.313881\n",
      "pred:\t tensor([172, 101, 387, 189, 347, 315, 178, 277, 277, 126], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 156 / 7800\tLoss:\t11.846785\n",
      "pred:\t tensor([14,  3, 11,  2, 11, 11, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([36,  3, 11,  2,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 312 / 7800\tLoss:\t13.001339\n",
      "pred:\t tensor([ 3,  5, 11,  2, 11,  3, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 3, 52, 11,  2,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 468 / 7800\tLoss:\t9.053352\n",
      "pred:\t tensor([14, 28,  3, 11,  2, 11,  3, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 14, 201, 309,  11,   2,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 624 / 7800\tLoss:\t9.884995\n",
      "pred:\t tensor([ 14,  40, 202,   3,  11,   3,  11,   2,  11,   3], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 38, 116, 202,   3, 207,   3,  11,   2,   0], device='cuda:0')\n",
      "\n",
      "iter 780 / 7800\tLoss:\t7.563014\n",
      "pred:\t tensor([ 3,  3, 24,  2,  5,  3,  5,  5,  5,  5], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([92,  3, 24,  2,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 936 / 7800\tLoss:\t6.467794\n",
      "pred:\t tensor([36,  3, 11,  2, 11,  3, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 36, 140,  11,   2,   0,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 1092 / 7800\tLoss:\t5.853202\n",
      "pred:\t tensor([15,  3, 11,  2, 11,  3, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([212,   3,  11,   2,   0,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 1248 / 7800\tLoss:\t6.573968\n",
      "pred:\t tensor([14,  3,  3, 11,  2, 11,  3, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([15, 89,  3, 11,  2,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 1404 / 7800\tLoss:\t5.921057\n",
      "pred:\t tensor([ 48,   3,  11, 148,  11,   2,  11,   3,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 48,   3, 147, 148,  11,   2,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 1560 / 7800\tLoss:\t6.408768\n",
      "pred:\t tensor([ 48, 164,   3, 227,  11,   2,  11,   3,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 48, 164,  35, 126,  11,   2,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 1716 / 7800\tLoss:\t5.946224\n",
      "pred:\t tensor([15,  3, 11,  2, 11, 11, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([15,  3, 11,  2,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 1872 / 7800\tLoss:\t5.726503\n",
      "pred:\t tensor([14,  3,  3, 11,  2, 11, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([14,  3,  3, 11,  2,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 2028 / 7800\tLoss:\t4.839216\n",
      "pred:\t tensor([14, 79, 41,  3, 11,  2, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 14, 375, 374,   3,  11,   2,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 2184 / 7800\tLoss:\t5.933374\n",
      "pred:\t tensor([176,   3,  24,   2,   5,   5,   5,   5,   5,   5], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([176,   3,  24,   2,   0,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 2340 / 7800\tLoss:\t4.205274\n",
      "pred:\t tensor([ 38,  78, 177,  11,   2,  11,  11,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 38, 283, 392,  11,   2,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 2496 / 7800\tLoss:\t4.032388\n",
      "pred:\t tensor([72,  9, 11,  2, 11, 11, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 3,  3, 11,  2,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 2652 / 7800\tLoss:\t6.281565\n",
      "pred:\t tensor([14, 79,  3, 11, 94, 11,  2, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([14, 39,  3, 42, 94, 11,  2,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 2808 / 7800\tLoss:\t3.783920\n",
      "pred:\t tensor([ 3,  5,  2, 11,  5, 11, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 2964 / 7800\tLoss:\t4.356105\n",
      "pred:\t tensor([36,  3, 11,  2, 11,  3, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([36,  3, 11,  2,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 3120 / 7800\tLoss:\t4.196994\n",
      "pred:\t tensor([  3,   9, 318,   3,   5,   2,   5,   5,   5,   5], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([  3, 107, 108,   3,   5,   2,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 3276 / 7800\tLoss:\t5.106712\n",
      "pred:\t tensor([ 3,  6,  5,  2, 11,  3, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([76,  6,  5,  2,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 3432 / 7800\tLoss:\t4.430221\n",
      "pred:\t tensor([176, 175,  24,  24,  24,  24,   5,   5,   5,   5], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([190, 145,   3,   2,   0,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 3588 / 7800\tLoss:\t3.052269\n",
      "pred:\t tensor([14, 79, 28, 41,  3, 11,  2, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([14, 79, 28, 41,  3, 11,  2,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 3744 / 7800\tLoss:\t2.621986\n",
      "pred:\t tensor([ 14, 171,   3,  11,   2,  11,  11,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([14, 70,  3, 11,  2,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 3900 / 7800\tLoss:\t3.241814\n",
      "pred:\t tensor([ 92,   3, 116,   3,  11,   2,  11,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([215,   3, 116,   3,  11,   2,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 4056 / 7800\tLoss:\t3.129902\n",
      "pred:\t tensor([ 38, 201,   3,  11,   2,  11,  11,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 14, 201,   3,  11,   2,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 4212 / 7800\tLoss:\t3.061626\n",
      "pred:\t tensor([ 83, 171, 409,  11,   2,  11, 171,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 83,  70, 409,  11,   2,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 4368 / 7800\tLoss:\t3.640471\n",
      "pred:\t tensor([266, 117,  24,   2,   5,   3,   5,   5,   5,   5], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([266, 118,  24,   2,   0,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 4524 / 7800\tLoss:\t3.535636\n",
      "pred:\t tensor([38,  3,  3, 37, 11,  2, 11,  5, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 38,  78, 193, 265,  11,   2,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 4680 / 7800\tLoss:\t2.997872\n",
      "pred:\t tensor([ 3, 11,  2, 11,  5, 11, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 4836 / 7800\tLoss:\t3.410772\n",
      "pred:\t tensor([ 38, 116,  11,   2,  11,  11,  11,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([38,  3, 11,  2,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 4992 / 7800\tLoss:\t2.802287\n",
      "pred:\t tensor([ 14,  28,   3,  11,   2,  11, 171,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 14,  28, 124,  11,   2,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 5148 / 7800\tLoss:\t2.882898\n",
      "pred:\t tensor([111,   3,   5,   2,   5,   5,  11,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([111,   3,  11,   2,   0,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 5304 / 7800\tLoss:\t2.745969\n",
      "pred:\t tensor([48,  3, 11,  2, 11, 11, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([48,  3, 11,  2,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 5460 / 7800\tLoss:\t2.650055\n",
      "pred:\t tensor([48,  3, 52, 52, 11,  2, 11,  3, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([48,  3, 34, 52, 11,  2,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 5616 / 7800\tLoss:\t2.728851\n",
      "pred:\t tensor([266,   3,  24,   2,   5,   5,   5,   5,   5,   5], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([266,   3,  24,   2,   0,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 5772 / 7800\tLoss:\t2.722663\n",
      "pred:\t tensor([162, 256,  11,   2,  11, 171,  11,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 47, 256,  11,   2,   0,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 5928 / 7800\tLoss:\t1.988753\n",
      "pred:\t tensor([90, 91,  3, 24,  2,  5,  5,  5,  5,  5], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([90, 91,  3, 24,  2,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 6084 / 7800\tLoss:\t2.668223\n",
      "pred:\t tensor([145, 242,  24,   2,   5,   5,   5,   5,   5,   5], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([145, 242,  24,   2,   0,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 6240 / 7800\tLoss:\t2.868902\n",
      "pred:\t tensor([ 36,  75,   3,  11,   2,  11, 171,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([36, 75,  3, 11,  2,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 6396 / 7800\tLoss:\t2.438774\n",
      "pred:\t tensor([  3, 350,  11,   2,  11, 171,  11,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([  3, 350,  11,   2,   0,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 6552 / 7800\tLoss:\t2.508089\n",
      "pred:\t tensor([167,   3,  24,   2,   5,   3,   5,   5,   5,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([167,   3,  24,   2,   0,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 6708 / 7800\tLoss:\t2.486957\n",
      "pred:\t tensor([ 14, 385,  58,  11,   2,  11, 171,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 14, 385,  58,  11,   2,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 6864 / 7800\tLoss:\t2.751235\n",
      "pred:\t tensor([ 38,  48,  78, 406, 146,  11,   2,  11, 171,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 38,  48,  78, 405, 146,  11,   2,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 7020 / 7800\tLoss:\t2.011924\n",
      "pred:\t tensor([ 15, 205,  11,   2,  11, 171,  11,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([15, 31, 11,  2,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 7176 / 7800\tLoss:\t2.228066\n",
      "pred:\t tensor([198, 108,  11,   2,  11, 171,  11,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([290,   3,  11,   2,   0,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 7332 / 7800\tLoss:\t2.023793\n",
      "pred:\t tensor([ 14,   9,  72,   3,  11,   2,  11, 171,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([362,   9,  72,   3,  11,   2,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 7488 / 7800\tLoss:\t2.080512\n",
      "pred:\t tensor([176,   9,   3,  24,   2,   5, 171,   5,   5,   5], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([176,  72,   3,  24,   2,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 7644 / 7800\tLoss:\t1.830984\n",
      "pred:\t tensor([ 48,   3,  34,   2,  11, 171,  11,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([48,  3, 11,  2,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training \n",
    "batch_size = 32\n",
    "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
    "\n",
    "embedding_dim = 250\n",
    "hidden_size = 128\n",
    "\n",
    "lr = 1e-3\n",
    "epoch = 50\n",
    "\n",
    "net = NMTRNN(vocab_eng.num_word, vocab_fra.num_word, embedding_dim, hidden_size)\n",
    "device = torch.device('cuda:0') # cuda:0 if you have gpu\n",
    "net = net.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss_list = []\n",
    "print_interval = len(train_iter)\n",
    "total_iter = epoch * len(train_iter)\n",
    "for e in range(epoch):\n",
    "  net.train()\n",
    "  for i, train_data in enumerate(train_iter):\n",
    "    train_data = [ds.to(device) for ds in train_data]\n",
    "    \n",
    "    loss, pred = net(*train_data)\n",
    "    \n",
    "    loss_list.append(loss.mean().detach())\n",
    "    optimizer.zero_grad()\n",
    "    loss.mean().backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    step = i + e * len(train_iter)\n",
    "    if step % print_interval == 0:\n",
    "      print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
    "      print('pred:\\t {}\\n'.format(pred[0].detach().to(device)))\n",
    "      print('tgt:\\t {}\\n'.format(train_data[2][0][1:].to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Curve**\n",
    "\n",
    "Plot and save the loss curve for the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save the loss curve figure in a file for the report\n",
    "plt.plot(np.arange(len(loss_list)), loss_list)\n",
    "plt.title('Loss Curve of Baseline')\n",
    "plt.savefig('./report_fig/loss_curve_baseline.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction Accuracy**\n",
    "\n",
    "Print out 5 prediction samples, and calculate the prediction accuracy over the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:\t ['fais', 'un', 'unk', '!', '.', '.']\n",
      "\n",
      "tgt:\t ['faites', 'un', 'unk', '!', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['merci', 'bien', '.', '.', '.', '.']\n",
      "\n",
      "tgt:\t ['unk', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['je', 'me', 'unk', 'à', 'toi', '.', '.']\n",
      "\n",
      "tgt:\t ['je', 'me', 'déteste', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['nous', 'unk', '.', '.', '.', '.']\n",
      "\n",
      "tgt:\t ['nous', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['ce', 'sont', 'des', 'unk', '.', '.', '.']\n",
      "\n",
      "tgt:\t ['ils', 'sont', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "Prediction Acc.: 0.7132\n"
     ]
    }
   ],
   "source": [
    "def comp_acc(pred, gt, valid_len, device):\n",
    "  N, T_gt = gt.shape[:2]\n",
    "  _, T_pr = pred.shape[:2]\n",
    "  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
    "  len_mask = torch.arange(T_gt).expand(N, T_gt).to(device)\n",
    "  len_mask = (len_mask < valid_len[:, None])\n",
    "  \n",
    "  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
    "  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
    "  return pred_acc\n",
    "  \n",
    "net.eval()# for predicting, we need turn off the net.training by calling eval()\n",
    "acc_list = []\n",
    "for i, train_data in enumerate(train_iter):\n",
    "  train_data = [ds.to(device) for ds in train_data]\n",
    "  pred = net(*train_data)\n",
    "  pred_acc = comp_acc(pred.detach().to(device), train_data[2].detach().to(device)[:, 1:], train_data[3].to(device), device)\n",
    "  acc_list.append(pred_acc)\n",
    "  if i < 5:# print 5 samples from 5 batches\n",
    "    pred = pred[0].detach().to(device)\n",
    "    pred_seq = []\n",
    "    for t in range(MAX_LEN+1):\n",
    "      pred_wd = vocab_fra.index2word[pred[t].item()] \n",
    "      if pred_wd != 'eos':\n",
    "        pred_seq.append(pred_wd)\n",
    "\n",
    "    print('pred:\\t {}\\n'.format(pred_seq))\n",
    "    print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].to(device)]))\n",
    "    \n",
    "print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TASK: Sequence to Sequence with LSTM and Attention\n",
    "\n",
    "In this task, you will improve the baseline model by implementing LSTM and the attention mechanism. The following skeleton codes will walk you through the required implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 TASK: LSTM Implementation\n",
    "\n",
    "LSTM eliminates the gradient explosion/vanishing problem. Its state and gate update at each time step can be summarized as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{State Update} &&& C_t &= F_t \\odot C_{t-1} + I_t \\odot \\tilde{C}_t \\\\\n",
    "&\\text{Hidden States} &&& H_t &= O_t \\odot \\text{tanh}(C_t) \\\\\n",
    "&\\text{Proposal} &&& \\tilde{C}_t &= \\text{tanh}( X_tW_{xc} + H_{t-1}W_{hc} + b_c ) \\\\\n",
    "&\\text{Input Gate} &&& I_t &= \\sigma( X_tW_{xi} + H_{t-1}W_{hi} + b_i ) \\\\\n",
    "&\\text{Forget Gate} &&& F_t &= \\sigma( X_tW_{xf} + H_{t-1}W_{hf} + b_f ) \\\\\n",
    "&\\text{Output Gate} &&& O_t &= \\sigma( X_tW_{xo} + H_{t-1}W_{ho} + b_o ) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "    \n",
    "**To-dos**\n",
    "- Implement the LSTM class:\n",
    "    - (5 points) Complete the initialization function *init_params()*. Refer to homework 1 for initialization script. Also feel free to try different initialization methods in *torch.nn.init*\n",
    "    - (10 points) Complete the function *lstm()* which performs the feed-forward pass of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, device):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.device = device\n",
    "        self.params = self.init_params(input_size, hidden_size, device)\n",
    "        self.hidden_size = hidden_size\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          input_size: int, feature dimension of input sequence\n",
    "          hidden_size: int, feature dimension of hidden state\n",
    "          device: torch.device()\n",
    "        \"\"\"\n",
    "  \n",
    "    def init_params(self, input_size, hidden_size, device):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          input_size: int, feature dimension of input sequence\n",
    "          hidden_size: int, feature dimension of hidden state\n",
    "\n",
    "        Outputs:\n",
    "          Weights for proposal: W_xc, W_hc, b_c\n",
    "          Weights for input gate: W_xi, W_hi, b_i\n",
    "          Weights for forget gate: W_xf, W_hf, b_f\n",
    "          Weights for output gate: W_xo, W_ho, b_o\n",
    "        \"\"\"\n",
    "\n",
    "        ###  START OF YOUR CODE ###\n",
    "        def normal(shape):\n",
    "            return nn.Parameter(torch.randn(shape, dtype=torch.float32, device=device)*0.01, requires_grad=True)\n",
    "        def three_rand_params():\n",
    "            return(normal([input_size, hidden_size]),\n",
    "                   normal([hidden_size, hidden_size]),\n",
    "                   nn.Parameter(torch.zeros([hidden_size], dtype=torch.float32, device=device), requires_grad=True))\n",
    "        W_xc, W_hc, b_c = three_rand_params()\n",
    "        W_xi, W_hi, b_i = three_rand_params()\n",
    "        W_xf, W_hf, b_f = three_rand_params()\n",
    "        W_xo, W_ho, b_o = three_rand_params()\n",
    "        ###  END OF YOUR CODE ###\n",
    "\n",
    "        params = [W_xc, W_hc, b_c, W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o]\n",
    "        return params\n",
    "    \n",
    "    def lstm(self, X, state):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          X: tuple of tensors (src, src_len). src, size (N, D_in) or (N, T, D_in), where N is the batch size,\n",
    "            T is the length of the sequence(s). src_len, size of (N,), is the valid length for each sequence.\n",
    "\n",
    "          state: tuple of tensors (h, c). h, size of (N, hidden_size) is the hidden state of LSTM. c, size of \n",
    "                (N, hidden_size), is the memory cell of the LSTM.\n",
    "\n",
    "        Outputs:\n",
    "          o: tensor of size (N, T, hidden_size).\n",
    "          state: the same as input state.\n",
    "        \"\"\"\n",
    "\n",
    "        ###  START OF YOUR CODE ###\n",
    "        device = self.device\n",
    "        [W_xc, W_hc, b_c, W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o] = self.params\n",
    "        H,C = state\n",
    "\n",
    "        X.to(device)\n",
    "        hidden_seq = []\n",
    "        bs, seq_sz, _ = X.size()\n",
    "        for t in range(seq_sz):\n",
    "            x_t = X[:, t, :]\n",
    "            I = torch.sigmoid( x_t@W_xi + torch.mm(H, W_hi) + b_i).to(device)\n",
    "            Fog = torch.sigmoid( torch.mm(x_t, W_xf) + torch.mm(H, W_hf) + b_f)\n",
    "            O = torch.sigmoid( torch.mm(x_t, W_xo) + torch.mm(H, W_ho) + b_o)\n",
    "            C_tilda = torch.tanh( torch.mm(x_t, W_xc) + torch.mm(H, W_hc) + b_c)\n",
    "            C = Fog*C + I*C_tilda\n",
    "            H = O*torch.tanh(C)\n",
    "            hidden_seq.append(H.unsqueeze(Dim.batch))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim = Dim.batch)\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(Dim.batch, Dim.seq).contiguous()\n",
    "        ###  END OF YOUR CODE ###\n",
    "\n",
    "        state = (H, C)\n",
    "        return hidden_seq, state\n",
    "  \n",
    "    def forward(self, inputs, state):\n",
    "        return self.lstm( inputs, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 TASK: Attention Mechanism\n",
    "\n",
    "Another improvement would be the Attention Mechanism. An example illustrating why applying attention mechanisms can improve the performance is shown in the picture below. An English sentence and its Chinese is visualized and aligned into blue boxes and red boxes, respectively. It can be seen that the Chinese character '她' has a long distance from its English counterpart, 'she'. Since only the final hidden state is passed to the decoder, it's hard for the baseline model to 'attend' to information a long time ago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./img/encoder-decoder-example.png\" width=\"600\"/>\n",
    "</div>\n",
    "Image source: https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Attention**\n",
    "\n",
    "    Given a query, $\\mathbf{q}$, and a set of (key, value) pairs, $\\{ \\mathbf{k}_i, \\mathbf{v}_i\\}^N$, the attention mechanism computes a weighted sum of values based on the normalized score obtained from the query and each key:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\mathbf{a} &= \\alpha(\\mathbf{q}, \\mathbf{K})\\text{, where } \\mathbf{K} = \\{\\mathbf{k}_i\\}^N \\\\\n",
    "    \\mathbf{b} &= \\text{softmax}(\\mathbf{a}) \\\\\n",
    "    \\mathbf{o} &= \\mathbf{b} \\cdot \\mathbf{V}\\text{, where } \\mathbf{V} = \\{\\mathbf{v}_i\\}^N\n",
    "    \\end{align*}\n",
    "    $$\n",
    "    The $\\alpha()$, which maps two vectors into a scalar, is the score function that can be chosen from a wide range, e.g. the cosine function, dot-product function, scaled dot-product funtion and etc.\n",
    "    \n",
    "- **Using Attention in seq2seq Models**\n",
    "\n",
    "    Given the whole sequence of hidden states, $\\{h_j\\}^{T_x}$, the decoder summarizes an averaged context vector, $c_i$, to update its current hidden state, $s_i$. Here, a() is the score function. $y_i$ is the target, $T_x$ denotes the valid length of the sequence. Formally, this process is computed as follows. Details can be found in the [paper](https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    \n",
    "    \\begin{align*}\n",
    "    e_{ij} &= a(s_{i-1}, h_j), j \\in \\{1, \\cdots, T_x\\} \\\\\n",
    "    \\mathbf{\\alpha}_i &= \\text{softmax}(\\mathbf{e}_{i}) \\\\\n",
    "    c_i &= \\sum^{T_x}_{j=1} \\alpha_{ij}h_j \\\\\n",
    "    s_i &= f(s_{i-1}, y_{i-1}, c_i)\n",
    "    \\end{align*}\n",
    "    \n",
    "    \n",
    "    \n",
    "- **Scaled Dot Product Attention**\n",
    "    - The scaled dot-product attention uses the score function as: $\\alpha(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q} \\mathbf{k}^T / \\sqrt{d}$, where $d$ is the dimension of query (also keys). The following figures visualizes this process in matrix form, in which $Q \\in \\mathcal{R}^{m\\times d_k}, \\mathbf{K} \\in \\mathcal{R}^{n \\times d_k}$, and $\\mathbf{V} \\in \\mathcal{R}^{n \\times d_v}$. A visualization of the attention calculation in matrix form of is shown below:\n",
    "\n",
    "    <div>\n",
    "    <img src=\"./img/self-attention-matrix-calculation-2.png\" width=\"600\"/>\n",
    "    </div>\n",
    "Image source: http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\n",
    "    \n",
    "**To-do**:\n",
    "- (5 points) Implement the masked_softmax() function.\n",
    "- (5 points) Implement the scaled dot-product attention in matrix multiplication, no **for loop** is allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_length, device):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    X: tensor of size (N, n, m) or (N, m)\n",
    "    valid_length: tensor of (N,)\n",
    "    \"\"\"\n",
    "    ###  START OF YOUR CODE ###\n",
    "    X.to(device)\n",
    "    valid_length.to(device)\n",
    "    softmax = nn.Softmax(2)\n",
    "    if valid_length is None:\n",
    "        return softmax(X)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        maxlen = shape[-1]\n",
    "        if X.ndim == 2:\n",
    "            valid_length = valid_length.unsqueeze(-1).expand_as(X).to(device)\n",
    "            idx = torch.arange(maxlen).unsqueeze(0).expand_as(X).to(device)\n",
    "        else:\n",
    "            if valid_length.ndim==1:\n",
    "                valid_length = valid_length.unsqueeze(-1).unsqueeze(-1).expand_as(X).to(device)\n",
    "            else:\n",
    "                valid_length = valid_length.unsqueeze(-1).expand_as(X).to(device)\n",
    "            idx = torch.arange(maxlen).unsqueeze(0).unsqueeze(1).expand_as(X).to(device)\n",
    "        mask = idx < valid_length\n",
    "    ###  END OF YOUR CODE ###\n",
    "    X[~mask] = -(1e7)\n",
    "    Y = softmax(X)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module): \n",
    "    def __init__(self, device):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "        self.device = device\n",
    "    def forward(self, query, key, value, valid_length=None):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "          query: tensor of size (B, n, d)\n",
    "          key: tensor of size (B, m, d)\n",
    "          value: tensor of size (B, m, dim_v)\n",
    "          valid_length: either (B, )\n",
    "\n",
    "          B is the batch_size, n is the number of queries, m is the number of <key, value> pairs,\n",
    "          d is the feature dimension of the query, and dim_v is the feature dimension of the value.\n",
    "\n",
    "        Outputs:\n",
    "          attention: tensor of size (B, n, dim_v), weighted sum of values\n",
    "        \"\"\"\n",
    "        ###  START OF YOUR CODE ###\n",
    "        device = self.device\n",
    "        query.to(device)\n",
    "        key.to(device)\n",
    "        value.to(device)\n",
    "        d = float(query.shape[-1])\n",
    "        scores = (torch.bmm(query, key.transpose(-2,-1)))/(d**0.5)\n",
    "        attention_weights = masked_softmax(scores, valid_length, device)\n",
    "        attention = torch.bmm(attention_weights, value)\n",
    "        ###  END OF YOUR CODE ###\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness Check for DotProductAttention\n",
    "\n",
    "You can copy and run the following snippet to check the whether your implementation of DotProductAttention is correct or not.\n",
    "\n",
    "```python\n",
    "att = DotProductAttention()\n",
    "keys = torch.zeros((2,10,2),dtype=torch.float)\n",
    "values = torch.arange((40), dtype=torch.float).view(1,10,4).repeat(2,1,1)\n",
    "atten(torch.zeros((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))\n",
    "\n",
    "\n",
    "Outputs:\n",
    "tensor([[[11.8604, 12.8604, 13.8604, 14.8604]],\n",
    "\n",
    "        [[12.7894, 13.7894, 14.7894, 15.7894]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **MLP Attention**\n",
    "\n",
    "    The MLP attention mechanism, where the score function is computed as: $ \\alpha(\\mathbf{q}, \\mathbf{k}) = \\mathbf{v}^T\\text{tanh}(W_k\\mathbf{k} + W_q\\mathbf{q})$, where $\\mathbf{v}, \\mathbf{W_q}\\text{, and }\\mathbf{v}$ are learnable parameters.\n",
    "    \n",
    "**To_do**:\n",
    "    - (5 points) Implement the MLP attention in matrix form, no **for loop** is allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPAttention(nn.Module):  \n",
    "    def __init__(self, d_v, d_k, d_q, device):\n",
    "        super(MLPAttention, self).__init__()\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          d_k: feature dimension of key\n",
    "          d_v: feature dimension of vector v\n",
    "          d_q: feature dimension of query\n",
    "        \"\"\"\n",
    "        ###  START OF YOUR CODE ###\n",
    "        self.W_k = nn.Parameter(torch.randn([d_v, d_k], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
    "        self.W_q = nn.Parameter(torch.randn([d_v, d_q], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
    "        self.v = nn.Parameter(torch.randn([1, d_v], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
    "        ###  END OF YOUR CODE ###\n",
    "\n",
    "    def forward(self, query, key, value, valid_length):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "          query: tensor of size (B, n, d)\n",
    "          key: tensor of size (B, m, d)\n",
    "          value: tensor of size (B, m, dim_v)\n",
    "          valid_length: either (B, )\n",
    "          n: d_q\n",
    "          m: d_k\n",
    "          d: d_v\n",
    "          B is the batch_size, n is the number of queries, m is the number of <key, value> pairs,\n",
    "          d is the feature dimension of the query, and dim_v is the feature dimension of the value.\n",
    "\n",
    "        Outputs:\n",
    "          attention: tensor of size (B, n, dim_v), weighted sum of values\n",
    "        \"\"\"\n",
    "        ###  START OF YOUR CODE ###\n",
    "        key = torch.matmul(key, self.W_k.t()).unsqueeze(0)\n",
    "        query = torch.matmul(query.to(device), self.W_q.t().unsqueeze(0))\n",
    "        features = query.unsqueeze(2) + key.unsqueeze(1)\n",
    "        scores = torch.matmul(features, self.v.t()).squeeze(-1).squeeze(0)\n",
    "        attention_weights = masked_softmax(scores, valid_length, device)\n",
    "        Y = torch.bmm(attention_weights, value.to(device))\n",
    "        ###  END OF YOUR CODE ###\n",
    "\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPAttention(nn.Module):  \n",
    "    def __init__(self, units, dropout, **kwargs):\n",
    "        super(MLPAttention, self).__init__(**kwargs)\n",
    "        # Use flatten=True to keep query's and key's 3-D shapes.\n",
    "        self.W_k = nn.Linear(2, units, bias=False)\n",
    "        self.W_q = nn.Linear(2, units, bias=False)\n",
    "        self.v = nn.Linear(8, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, valid_length):\n",
    "        query, key = self.W_q(query), self.W_k(key)\n",
    "        # expand query to (batch_size, #querys, 1, units), and key to\n",
    "        # (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.\n",
    "        features = query.unsqueeze(2) + key.unsqueeze(1)\n",
    "        scores = self.v(features).squeeze(-1) \n",
    "        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n",
    "        return torch.bmm(attention_weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 TASK: Encoder-Decoder\n",
    "\n",
    "\n",
    "Build a seq2seq model with LSTM and attention:\n",
    "- (5 points) Complete the Encoder forward() function.\n",
    "- (10 points) Complete the Decoder forward() function.\n",
    "- (10 points) Complete the class NMTLSTM. If you could not implement the class LSTM, you can use torch.nn.LSTM to help you finish this task.\n",
    "- Implement the training script.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, device):\n",
    "        super(Encoder, self).__init__()\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "          vocab_size: int, the number of words in the vocabulary\n",
    "          embedding_dim: int, dimension of the word embedding\n",
    "          hidden_size: int, dimension of vallina RNN\n",
    "        \"\"\"\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = LSTM(embedding_dim, hidden_size, device)\n",
    "        self.output_emb = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, sources, valid_len):\n",
    "        ###  START OF YOUR CODE ###\n",
    "        word_embedded = self.embedding(sources)\n",
    "        N = word_embedded.shape[0]\n",
    "        h = sources.new_zeros(N, self.hidden_size).float().to(device)\n",
    "        c = sources.new_zeros(N, self.hidden_size).float().to(device)\n",
    "        state = (h,c)\n",
    "        out, state = self.rnn(word_embedded, state)\n",
    "        ###  END OF YOUR CODE ###\n",
    "\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, device):\n",
    "        super(Decoder, self).__init__()\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "          vocab_size: int, the number of words in the vocabulary\n",
    "          embedding_dim: int, dimension of the word embedding\n",
    "          hidden_size: int, dimension of vallina RNN\n",
    "        \"\"\"\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = LSTM(embedding_dim+hidden_size, hidden_size, device)\n",
    "        self.att = DotProductAttention(device)\n",
    "        self.output_emb = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, state, target, valid_len):\n",
    "        ###  START OF YOUR CODE ###\n",
    "        word_embedded = self.embedding(target)\n",
    "        outputs = []\n",
    "        e_h, e_c = state\n",
    "        k_v = e_h.unsqueeze(1)\n",
    "        bs, seq_sz, _ = word_embedded.size()\n",
    "        for t in range(seq_sz):\n",
    "            x_t = word_embedded[:, t, :]\n",
    "            h, c = state\n",
    "            query = h.unsqueeze(1)\n",
    "            context = self.att(query, k_v, k_v, valid_len)\n",
    "            # Concatenate on the feature dimension\n",
    "            x_t = torch.cat((context, x_t.unsqueeze(1)), dim=-1)\n",
    "            out, state = self.rnn( x_t, state)\n",
    "            outputs.append(out)\n",
    "        pred = self.output_emb(torch.cat(outputs, dim=1))\n",
    "        ###  END OF YOUR CODE ###\n",
    "        return pred, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTLSTM(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size, device):\n",
    "        super(NMTLSTM, self).__init__()\n",
    "        self.enc = Encoder(src_vocab_size, embedding_dim, hidden_size, device)\n",
    "        self.dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size, device)\n",
    "\n",
    "    def forward(self, src, src_len, tgt, tgt_len):\n",
    "        output, h = self.enc(src, src_len)\n",
    "        T = tgt.shape[1]\n",
    "\n",
    "        if self.training:\n",
    "            pred, _ = self.dec(h, tgt, tgt_len)\n",
    "            loss = 0\n",
    "            for t in range(T-1):\n",
    "                # target sequence should shift by one time-step, because we are predicting the next word\n",
    "                loss = loss + F.nll_loss(F.log_softmax(pred[:, t]), tgt[:, t+1], ignore_index=0)\n",
    "                \n",
    "\n",
    "            return loss, pred.argmax(dim=-1)\n",
    "\n",
    "        else:\n",
    "            \"\"\"\n",
    "            When predicting a sequence given the 'bos' token, the input for the next step is the predicted\n",
    "            token from the previous time step.\n",
    "            \"\"\"\n",
    "            inputs = tgt[:, :1]\n",
    "            preds = []\n",
    "            for t in range(MAX_LEN+1): # plus the 'eos' token\n",
    "                pred, h = self.dec(h, inputs, tgt_len)\n",
    "                inputs = pred.argmax(dim=-1)\n",
    "                preds.append(pred)\n",
    "            pred = torch.cat(preds, dim=1).argmax(dim=-1)\n",
    "            return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 / 7800\tLoss:\t48.306084\n",
      "pred:\t tensor([ 32, 246, 219, 209, 214, 246, 303, 219, 219, 219], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 38, 116, 202,   3,  11,   2,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 780 / 7800\tLoss:\t3.606470\n",
      "pred:\t tensor([14,  3, 11,  2,  2, 11, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([14,  3, 11,  2,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 1560 / 7800\tLoss:\t2.896785\n",
      "pred:\t tensor([ 14, 116, 179,   3,  37,   3,  11,   2,   2,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 14, 116, 179, 369,  37,   3,  11,   2,   0], device='cuda:0')\n",
      "\n",
      "iter 2340 / 7800\tLoss:\t2.102764\n",
      "pred:\t tensor([ 38, 357,   3, 409,  11,   2,   2,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 38, 169, 113, 409,  11,   2,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 3120 / 7800\tLoss:\t2.021135\n",
      "pred:\t tensor([ 52, 388,   3,  24,   2,   2,   3,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 52, 388,   3,  24,   2,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 3900 / 7800\tLoss:\t1.602879\n",
      "pred:\t tensor([141, 147,   5,   2,   2,   3,  11,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([241, 146,   5,   2,   0,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 4680 / 7800\tLoss:\t1.456299\n",
      "pred:\t tensor([52, 40,  3, 11,  2,  2, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([52, 40,  3, 11,  2,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 5460 / 7800\tLoss:\t1.490859\n",
      "pred:\t tensor([ 14,  28, 128,  11,   2,   2,   3,   3,   3,   3], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 14,  28, 128,  11,   2,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 6240 / 7800\tLoss:\t1.444488\n",
      "pred:\t tensor([14, 28, 11,  2,  2,  3,  3,  3,  3,  3], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([14,  3, 11,  2,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 7020 / 7800\tLoss:\t1.605405\n",
      "pred:\t tensor([36, 35,  8, 11,  2,  2,  3,  3,  3,  3], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([36, 35,  8, 11,  2,  0,  0,  0,  0], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # training \n",
    "batch_size = 32+32\n",
    "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
    "\n",
    "embedding_dim = 250\n",
    "hidden_size = 128+32\n",
    "\n",
    "lr = (1e-2)*5\n",
    "epoch = 50+50\n",
    "device = torch.device('cuda:0') # cuda:0 if you have gpu\n",
    "net = NMTLSTM(vocab_eng.num_word, vocab_fra.num_word, embedding_dim, hidden_size, device)\n",
    "net = net.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss_list = []\n",
    "print_interval = len(train_iter)*10\n",
    "total_iter = epoch * len(train_iter)\n",
    "for e in range(epoch):\n",
    "    net.train()\n",
    "    for i, train_data in enumerate(train_iter):\n",
    "        train_data = [ds.to(device) for ds in train_data]\n",
    "\n",
    "        loss, pred = net(*train_data)\n",
    "\n",
    "        loss_list.append(loss.mean().detach())\n",
    "        optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step = i + e * len(train_iter)\n",
    "        if step % print_interval == 0:\n",
    "            print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
    "            print('pred:\\t {}\\n'.format(pred[0].detach().to(device)))\n",
    "            print('tgt:\\t {}\\n'.format(train_data[2][0][1:].to(device)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Report\n",
    "\n",
    "- (5 points) Generate predictions with NMTLSTM. During predicting, the decoder only sees the 'bos' token and hidden states from encoder.\n",
    "- (5 points) Plot the loss curve, report the prediction accuracy and print out several (~5) prediction samples together with corresponding targets. Bonus points will be assigned depending on the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save the loss curve figure in a file for the report\n",
    "plt.plot(np.arange(len(loss_list)), loss_list)\n",
    "plt.title('Loss Curve of lstm seq2sq model')\n",
    "plt.savefig('./report_fig/loss_curve_lstm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:\t [\"c'est\", 'bien', 'pour', 'vous', '.', 'unk', '.']\n",
      "\n",
      "tgt:\t [\"c'est\", 'bien', 'pour', 'vous', '.', 'eos', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['unk', 'mon', 'unk', '!', 'unk', '.']\n",
      "\n",
      "tgt:\t ['unk', 'mon', 'unk', '!', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['venez', 'chez', 'moi', '!', 'unk', '.']\n",
      "\n",
      "tgt:\t ['viens', 'chez', 'nous', '!', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['prends', 'un', 'unk', '.', 'unk', '.']\n",
      "\n",
      "tgt:\t ['prenez', 'un', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['soyez', 'calme', '!', 'unk', '.']\n",
      "\n",
      "tgt:\t ['soyez', 'unk', '!', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "Prediction Acc.: 0.7575\n"
     ]
    }
   ],
   "source": [
    "# Prediction Accuracy\n",
    "# Print out 5 prediction samples, and calculate the prediction accuracy over the training dataset.\n",
    "def comp_acc(pred, gt, valid_len, device):\n",
    "    N, T_gt = gt.shape[:2]\n",
    "    _, T_pr = pred.shape[:2]\n",
    "    assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
    "    len_mask = torch.arange(T_gt).expand(N, T_gt).to(device)\n",
    "    len_mask = (len_mask < valid_len[:, None])\n",
    "\n",
    "    pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
    "    pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
    "    return pred_acc\n",
    "  \n",
    "net.eval()# for predicting, we need turn off the net.training by calling eval()\n",
    "acc_list = []\n",
    "for i, train_data in enumerate(train_iter):\n",
    "    train_data = [ds.to(device) for ds in train_data]\n",
    "    pred = net(*train_data)\n",
    "    pred_acc = comp_acc(pred.detach().to(device), train_data[2].detach().to(device)[:, 1:], train_data[3].to(device), device)\n",
    "    acc_list.append(pred_acc)\n",
    "    if i < 5:# print 5 samples from 5 batches\n",
    "        pred = pred[0].detach().to(device)\n",
    "        pred_seq = []\n",
    "        for t in range(MAX_LEN+1):\n",
    "            pred_wd = vocab_fra.index2word[pred[t].item()] \n",
    "            if pred_wd != 'eos':\n",
    "                pred_seq.append(pred_wd)\n",
    "\n",
    "        print('pred:\\t {}\\n'.format(pred_seq))\n",
    "        print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].to(device)]))\n",
    "    \n",
    "print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TASK: Transformer\n",
    "\n",
    "Recurrent Neural Networks can capture the long-range, variable-length sequential information, but updating the current state relies on the previous states. Thus it suffers from the inability of parallelization within the sequence. In contrast, CNNs are easy to parallelize while they cannot capture sequential dependency within variable-length sequences. Transformer resolves this dilemma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Transformer Components Implementation\n",
    "\n",
    "The following codes will walk you through each of them. The original paper can be found [here](https://arxiv.org/pdf/1706.03762.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-Head Self-Attention**\n",
    "\n",
    "\n",
    "Note that each head in the multi-head self-attention is the scaled dot-product attention which you have already implemented in Section 3.2. To be specific, the steps in the multi-head attention can be summarzied by the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   1. The multi-head self-attention takes the initinal query $Q$, key $K$, and value $V$ as input. Note that, if not provided specifically, usually these are set to the same input embeddings $X=Q=K=V$ initially.\n",
    "   \n",
    "   1. Then, a linear projection is applied to $Q,K,V$ sepearately for each head $i=1,\\dots,h$. \n",
    "      $$\n",
    "   Q_i = QW^{Q}_i, K_i = KW^{K}_i, V_i = VW^{V}_i, i \\in [0, \\dots, h-1],\n",
    "   $$\n",
    "   \n",
    "   where $W^Q_i \\in \\mathcal{R}^{d_{model} \\times d_k}, W^K_i \\in \\mathcal{R}^{d_{model} \\times d_k}\\text{, and } W^V_i \\in \\mathcal{R}^{d_{model} \\times d_v}$\n",
    "     \n",
    "   1. Apply the scaled dot-product attention to each of these projected set of queries, keys and values:\n",
    "   $$\n",
    "   \\text{head}_i = \\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}(\\frac{Q_iK^T_i}{\\sqrt{d_k}})V_i\n",
    "   $$\n",
    "   \n",
    "   1. Concatenate all the heads together and project it with another learned linear projections: \n",
    "   \n",
    "   $$\n",
    "   \\text{O} = \\text{Concate(head}_1, \\dots, \\text{head}_h) \\\\\n",
    "   \\text{MultiHead}(Q, K, V) = \\text{O}W^o, \\hspace{10mm} \\text{where } W^o \\in \\mathcal{R}^{{hd_v} \\times d_{model}}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good visualization from this blog is shown below. Transformer stacks several multi-head attention modules together, thus the first multi-head layer has input from the dataset, an additional embedding layer is needed to project the input sequence into appropriate dimensions. Otherwise, the output from the layer right below is directly used as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"./img/transformer_multi-headed_self-attention-recap.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "Image source: http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To-dos**:\n",
    "- Complete the class MultiHeadAttention:\n",
    " - (5 points) Complete the __inti__() function, where the linear mappings for query, key, and values should be created.\n",
    " - (5 points) Complemte the forward() function, where the multi-head attention is performed. Note that, for the decoder in the Transformer, when applying self-attention, the sequential queries **cannot** attend to those at later time steps. For example, in a sequence, query entry at time step 5 can only observe the first 5 entries. You need to use masked softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, num_heads, device, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          d_model: int, the same d_model in paper, feature dimension of query/key/values\n",
    "          d_k: int, feature projected dimension of query/key/value, we follow the setting in the paper, where d_v=d_k=d_q\n",
    "          num_heads: int, number of heads used for this MultiHeadAttention\n",
    "        \"\"\"\n",
    "        self.num_heads = num_heads\n",
    "        self.attn = DotProductAttention(device)\n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "        ###  START OF YOUR CODE ###\n",
    "        self.W_k = nn.Parameter(torch.randn([d_k, d_model], dtype=torch.float32, device=device)*0.01, requires_grad=True)\n",
    "        self.b_k = nn.Parameter(torch.zeros([d_model], dtype=torch.float32, device=device), requires_grad=False)\n",
    "        self.W_q = nn.Parameter(torch.randn([d_k, d_model], dtype=torch.float32, device=device)*0.01, requires_grad=True)\n",
    "        self.b_q = nn.Parameter(torch.zeros([d_model], dtype=torch.float32, device=device), requires_grad=False)\n",
    "        self.W_v = nn.Parameter(torch.randn([d_k, d_model], dtype=torch.float32, device=device)*0.01, requires_grad=True)\n",
    "        self.b_v = nn.Parameter(torch.zeros([d_model], dtype=torch.float32, device=device), requires_grad=False)\n",
    "        self.W_o = nn.Parameter(torch.randn([d_model, d_model], dtype=torch.float32, device=device)*0.01, requires_grad=True)\n",
    "        self.b_o = nn.Parameter(torch.zeros([d_model], dtype=torch.float32, device=device), requires_grad=False)\n",
    "        ###  END OF YOUR CODE ###\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value, valid_length):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "          query: tensor of size (B, T, d_q)\n",
    "          key: tensor of size (B, m, d_k)\n",
    "          value: tensor of size (B, m, d_v)\n",
    "          valid_length: either (B, )\n",
    "\n",
    "          B is the batch_size, T is length of sequence, d_q, d_k, and d_v are the feature dimensions of query,\n",
    "          key, and value. In the paper, d_q = d_k = d_v.\n",
    "\n",
    "        Outputs:\n",
    "          attention\n",
    "          \"\"\"\n",
    "        ###  START OF YOUR CODE ###\n",
    "        query.to(device)\n",
    "        key.to(device)\n",
    "        value.to(device)\n",
    "        if valid_length is not None:\n",
    "            valid_length.to(device)\n",
    "        bs = query.size(0)\n",
    "        def transpose_qkv(X, num_heads):\n",
    "            # Original X shape: (batch_size, seq_len, hidden_size * num_heads),\n",
    "            # -1 means inferring its value, after first reshape, X shape:\n",
    "            # (batch_size, seq_len, num_heads, hidden_size)\n",
    "            X = X.contiguous().reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "            # After transpose, X shape: (batch_size, num_heads, seq_len, hidden_size)\n",
    "            X = X.transpose(2, 1)\n",
    "            # Merge the first two dimensions. Use reverse=True to infer shape from\n",
    "            # right to left.\n",
    "            # output shape: (batch_size * num_heads, seq_len, hidden_size)\n",
    "            output = X.contiguous().reshape(-1, X.shape[2], X.shape[3])\n",
    "            return output         \n",
    "        #perform linear operation and split into h heads\n",
    "        k = transpose_qkv(torch.matmul(key, self.W_k)+self.b_k, self.num_heads)\n",
    "        q = transpose_qkv(torch.matmul(query, self.W_q)+self.b_q, self.num_heads)\n",
    "        v = transpose_qkv(torch.matmul(value, self.W_v)+self.b_v, self.num_heads)\n",
    "        if valid_length is not None:\n",
    "            if valid_length.ndim == 1:\n",
    "                valid_length = valid_length.repeat(self.num_heads)\n",
    "            else:\n",
    "                valid_length = valid_length.repeat((self.num_heads, 1))\n",
    "        # calculate attention using function we will define next\n",
    "        scores = self.attn(q, k, v, valid_length)\n",
    "        scores = scores.contiguous().reshape(bs, self.num_heads, scores.shape[1], scores.shape[2])\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().reshape(bs, -1, self.d_model)\n",
    "        attention = torch.matmul(concat, self.W_o)+self.b_o\n",
    "        ###  END OF YOUR CODE ###\n",
    "\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Network\n",
    "\n",
    "Another key component in the Transformer block is called position-wise feed-forward network (FFN). It's called position-wise FFN because the linear mapping is applied to each position separately and identically. For example, for an embedded input of size $N \\times T \\times D_{in}$, there are $N*T$ vectors of dimension $D_{in}$. If we apply one layer position-wise FFN with weights of size $D_{in} \\times D_{out}$. The linear projection will be applied to each of the $N*T$ vectors separately and identically. Thus, the output would have size $N \\times T \\times D_{out}$. \n",
    "\n",
    "Transformer stacks two layers of position-wsie FFN together, with a ReLU activation in between:\n",
    "\n",
    "$$\n",
    "\\text{PositionWiseFFN}(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "**To-dos**:\n",
    "\n",
    "    - Complete the class PositionWiseFFN:\n",
    "        - (5 points) Complete the __init__() function, where two position-wise FFN should be created.\n",
    "        - (5 points) Complete the forward() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, input_size, ffn_l1_size, ffn_l2_size, device):\n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          input_size: int, feature dimension of the input\n",
    "          fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
    "          fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
    "        \"\"\"\n",
    "        self.W_1 = nn.Parameter(torch.randn([input_size, ffn_l1_size], dtype=torch.float32, device=device)*0.01, requires_grad=True)\n",
    "        self.b_1 = nn.Parameter(torch.zeros([ffn_l1_size], dtype=torch.float32, device=device), requires_grad=True)\n",
    "        self.W_2 = nn.Parameter(torch.randn([ffn_l1_size, ffn_l2_size], dtype=torch.float32, device=device)*0.01, requires_grad=True)\n",
    "        self.b_2 = nn.Parameter(torch.zeros([ffn_l2_size], dtype=torch.float32, device=device), requires_grad=True)\n",
    "        self.device = device\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "          X: tensor of size (N, T, D_in)\n",
    "        Output:\n",
    "          o: tensor of size (N, T, D_out)\n",
    "        \"\"\"\n",
    "        ###  START OF YOUR CODE ###\n",
    "        X.to(self.device)\n",
    "        result = F.relu((torch.matmul(X, self.W_1)+self.b_1))\n",
    "#         result[result<=0.0] = 0.0\n",
    "        result = torch.matmul(result.to(self.device), self.W_2)+self.b_2\n",
    "        ###  END OF YOUR CODE ###\n",
    "        return result    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "Replacing RNNs with the multi-head attention layer and applying the position-wise feed-forward network makes the computation sofar parallelizable since these modules compute the output of each item in the sequence independently. But, to understand the meaning of a sequence, the sequential information is very important. To this end, Transformer model proposes the positional encoding:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "PE_{(pos, 2i)} &= sin(pos / 10000^{2i/d_{model}}) \\\\\n",
    "PE_{(pos, 2i+1)} &= cos(pos / 10000^{2i/d_{model}}) \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "An example borrowed from this [blog](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/) can give an ituition how this positional encoding works. Suppose you want to encode the number from $0$ to $8$ using binary encoding, the result would like this:\n",
    "$$\n",
    "\\begin{align*}\n",
    "0: && 0  0  0 \\\\\n",
    "1: && 0  0  1 \\\\\n",
    "2: && 0  1  0 \\\\\n",
    "3: && 0  1  1 \\\\\n",
    "4: && 1  0  0 \\\\\n",
    "5: && 1  0  1 \\\\\n",
    "6: && 1  1  0 \\\\\n",
    "7: && 1  1  1 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Note the frequency of ones in each digit is different. Thus, words at different locations will have different embedding features (digits in the example). The figure below visualized a position encoding matrix of dimension $\\mathcal{R}^{50 \\times 128}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"./img/positional_encoding.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "Image source: https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To-dos**:\n",
    "\n",
    "- Complete the class PositionalEncoding:\n",
    "    - (7 points) Complete the __init__() function, where the tensor $PE$ should be created.\n",
    "    - (3 points) Complete the forward() function.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=1000, device='cuda:0'):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          dim: feature dimension of the positional encoding\n",
    "        \"\"\"\n",
    "        ###  START OF YOUR CODE ###\n",
    "        self.device = device\n",
    "        self.d_model = dim\n",
    "        self.P = nn.Parameter(torch.zeros([1, max_len, dim], dtype=torch.float32, device=device), requires_grad=False)\n",
    "        m = torch.arange(0, max_len).contiguous().reshape(-1,1)/torch.pow(10000.0, torch.arange(0, dim, 2).float()/dim)\n",
    "        self.P[:, :, 0::2] = torch.sin(m)\n",
    "        self.P[:, :, 1::2] = torch.cos(m)\n",
    "        ###  END OF YOUR CODE ###\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          X: tensor of size (N, T, D_in)\n",
    "        Output:\n",
    "          Y: tensor of the same size of X\n",
    "        \"\"\"\n",
    "        ###  START OF YOUR CODE ###\n",
    "        X.to(self.device)\n",
    "        Y = X+self.P[:, :X.shape[1], :] \n",
    "        ###  END OF YOUR CODE ###\n",
    "        return Y\n",
    "# device = torch.device('cuda:0')\n",
    "# pe = PositionalEncoding(20, device=device)\n",
    "# Y = pe(torch.zeros((1, 100, 20)).to(device))\n",
    "# plt.plot(torch.arange(100), Y[0, :, 4].cpu())\n",
    "# plt.plot(torch.arange(100), Y[0, :, 5].cpu())\n",
    "# plt.plot(torch.arange(100), Y[0, :, 6].cpu())\n",
    "# plt.plot(torch.arange(100), Y[0, :, 7].cpu())\n",
    "# plt.legend([\"dim %d\" % p for p in [4, 5, 6, 7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add and Norm\n",
    "\n",
    "Transformer uses a residual connection followed by a layer normalization layer to connect the inputs and outputs of other layers. To be specific, an \"add and norm\" layer is appended after each multi-head attention layer and the position-wise FFN layer. *The code for AddNorm Layer is given as below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, dropout, embedding_size, device):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout).to(device)\n",
    "        self.norm = nn.LayerNorm(embedding_size).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        Y.to(self.device)\n",
    "        X.to(self.device)\n",
    "        return self.norm(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Decoder\n",
    "\n",
    "The following figure gives a simple example of how the Transformer is built on these components introduced above. It's easy to see that the encoder of the Transformer consists of several identical encoder blocks, and so does the decoder. The codes of the encoder and decoder block are given below by using the components you have implemented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./img/transformer_resideual_layer_norm_3.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "Image source: http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To-dos**:\n",
    "    - (10 points) Complete the forward() function for the EncoderBlock.\n",
    "    - (10 points) Complete the forward() function for the DecoderBlock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_k, ffn_l1_size, ffn_l2_size, num_heads, dropout, device):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          d_model: int, feature dimension of query/key/value\n",
    "          d_k: int, feature projected dimension of query/key/value, we follow the setting in the paper, where d_v=d_k=d_q\n",
    "          fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
    "          fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
    "          num_heads: int, number of head for multi-head attention layer.\n",
    "          dropout: dropout probability for dropout layer.\n",
    "\n",
    "        \"\"\"\n",
    "        self.attention = MultiHeadAttention(d_model, d_k, num_heads, device)\n",
    "        self.addnorm_1 = AddNorm(dropout, d_model, device)\n",
    "        self.ffn = PositionWiseFFN(d_model, ffn_l1_size, ffn_l2_size, device)\n",
    "        self.addnorm_2 = AddNorm(dropout, d_model, device)\n",
    "        self.device = device\n",
    "    def forward(self, X, valid_length):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          X: tensor of size (N, T, D), embedded input sequences\n",
    "          valid_length: tensor of size (N,), valid lengths for each sequence\n",
    "        \"\"\"\n",
    "        ###  START OF YOUR CODE ###\n",
    "        X.to(self.device)\n",
    "        valid_length.to(self.device)\n",
    "        Y = self.addnorm_1(X, self.attention(X, X, X, valid_length))\n",
    "        Y = self.addnorm_2(Y, self.ffn(Y))\n",
    "        ###  END OF YOUR CODE ###\n",
    "\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_k, ffn_l1_size, ffn_l2_size, num_heads, dropout, i, device, **kwargs):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          d_model: int, feature dimension of query/key/value\n",
    "          d_k: int, feature projected dimension of query/key/value, we follow the setting in the paper, where d_v=d_k=d_q\n",
    "          fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
    "          fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
    "          num_heads: int, number of head for multi-head attention layer.\n",
    "          dropout: dropout probability for dropout layer.\n",
    "\n",
    "        \"\"\"\n",
    "        self.i = i\n",
    "        self.attention_1 = MultiHeadAttention(d_model, d_k, num_heads, device)\n",
    "        self.addnorm_1 =  AddNorm(dropout, d_model, device)\n",
    "        self.attention_2 = MultiHeadAttention(d_model, d_k, num_heads, device)\n",
    "        self.addnorm_2 = AddNorm(dropout, d_model, device)\n",
    "        self.ffn = PositionWiseFFN(d_model, ffn_l1_size, ffn_l2_size, device)\n",
    "        self.addnorm_3 = AddNorm(dropout, d_model, device)\n",
    "        \n",
    "    def forward(self, X, state):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          X: tensor of size (N, T, D), embedded input sequences\n",
    "          **kwargs: other arguments you think is necessary for implementation\n",
    "        Outputs:\n",
    "          Y: tensor of size (N, T, D_out)\n",
    "\n",
    "          Feel free to output variables if necessary.\n",
    "        \"\"\"\n",
    "        ###  START OF YOUR CODE ###\n",
    "        enc_outputs, enc_valid_length = state[0], state[1]\n",
    "        # state[2][i] contains the past queries for this block\n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X\n",
    "        else:\n",
    "            key_values = torch.cat((state[2][self.i], X), dim=1)        \n",
    "        state[2][self.i] = key_values\n",
    "        \n",
    "        batch_size, seq_len, _ = X.shape\n",
    "        # Shape: (batch_size, seq_len), the values in the j-th column\n",
    "        # are j+1\n",
    "        valid_length = torch.arange(1, seq_len+1).repeat((batch_size, 1))\n",
    "        X2 = self.attention_1(X, key_values, key_values, valid_length)\n",
    "        Y = self.addnorm_1(X, X2)\n",
    "        Y2 = self.attention_2(Y, enc_outputs, enc_outputs, enc_valid_length)\n",
    "#         Y2 = self.linear2(F.relu(self.linear1(Y)))\n",
    "        Z = self.addnorm_2(Y, Y2)\n",
    "        ###  END OF YOUR CODE ###\n",
    "        return self.addnorm_3(Z, self.ffn(Z)), state   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Transformer  Implementation\n",
    "\n",
    "By stacking two encoder blocks and two decoder blocks, build the Transformer using the above components. \n",
    "\n",
    "**To-dos**:\n",
    "- Implement the Encoder of Transformer:\n",
    " - (10 points) Complete the __init__() function with a word embedding layer and several EncoderBlocks.\n",
    " - (5 points) Complete the forward() function\n",
    "- Implement the Decoder of Transformer\n",
    " - (10 points) Complete the __init__() function\n",
    " - (5 points) Complete the forward() function\n",
    " - (10 points) Implement the Transformer\n",
    "- Finish training of the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, ffn_l1_size, ffn_l2_size,\n",
    "               num_heads, num_layers, dropout, device):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          d_model: int, feature dimension of query/key/value\n",
    "          fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
    "          fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
    "          num_heads: int, number of head for multi-head attention layer.\n",
    "          dropout: dropout probability for dropout layer.\n",
    "          num_layers: number of encoder blocks\n",
    "        \"\"\"\n",
    "        ###  START OF YOUR CODE ###\n",
    "        self.embedding_size = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model).to(device)\n",
    "        self.pos_encoding = PositionalEncoding(int(d_model))\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers += [EncoderBlock(d_model, d_model, ffn_l1_size, ffn_l2_size, num_heads, dropout, device)]\n",
    "        self.blks = nn.Sequential(*layers)\n",
    "        self.device = device\n",
    "        ###  END OF YOUR CODE ###\n",
    "        \n",
    "    def forward(self, X, valid_length):\n",
    "        ###  START OF YOUR CODE ###\n",
    "        word_embedded = self.embedding(X)\n",
    "        X = self.pos_encoding(self.embedding(X) * ((self.embedding_size)**0.5))\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_length)\n",
    "        ###  END OF YOUR CODE ###\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, ffn_l1_size, ffn_l2_size,\n",
    "             num_heads, num_layers, dropout, device):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          d_model: int, feature dimension of query/key/value\n",
    "          fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
    "          fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
    "          num_heads: int, number of head for multi-head attention layer.\n",
    "          dropout: dropout probability for dropout layer.\n",
    "          num_layers: number of decoder blocks\n",
    "        \"\"\"\n",
    "        ###  START OF YOUR CODE ###\n",
    "        self.embedding_size = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model).to(device)\n",
    "        self.pos_encoding = PositionalEncoding(int(d_model))\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers += [DecoderBlock(d_model, d_model, ffn_l1_size, ffn_l2_size, num_heads, dropout, i, device)]\n",
    "        self.blks = nn.Sequential(*layers)\n",
    "        self.W_o = nn.Parameter(torch.randn([d_model, vocab_size], dtype=torch.float32, device=device)*0.01, requires_grad=True)\n",
    "        self.b_o = nn.Parameter(torch.zeros([vocab_size], dtype=torch.float32, device=device), requires_grad=True)\n",
    "        self.device = device\n",
    "        ###  END OF YOUR CODE ###\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          X: tensor of size (N, T, D), embedded input sequences\n",
    "          valid_length: tensor of size (N,), valid lengths for each sequence\n",
    "        \"\"\"\n",
    "        ###  START OF YOUR CODE ###\n",
    "        word_embedded = self.embedding(X)\n",
    "        X = self.pos_encoding(word_embedded * ((self.embedding_size)**0.5))\n",
    "        for blk in self.blks:\n",
    "            X, state = blk(X, state)\n",
    "        X = torch.matmul(X, self.W_o)+self.b_o\n",
    "        ###  END OF YOUR CODE ###\n",
    "\n",
    "        return X, state\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size, num_heads, num_layers, dropout, device):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.enc = TransformerEncoder(src_vocab_size, embedding_dim, hidden_size, embedding_dim, num_heads, num_layers, dropout, device)\n",
    "        self.dec = TransformerDecoder(tgt_vocab_size, embedding_dim, hidden_size, embedding_dim, num_heads, num_layers, dropout, device)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, src, src_len, tgt, tgt_len):\n",
    "        output = self.enc(src, src_len)\n",
    "        T = tgt.shape[1]\n",
    "        h = []\n",
    "        h.append(output)\n",
    "        h.append(tgt_len)\n",
    "        q = []\n",
    "        for qi in range(self.num_layers):\n",
    "            q.append(None)\n",
    "        h.append(q)\n",
    "        if self.training:\n",
    "            pred, h = self.dec(tgt, h)\n",
    "            loss = 0\n",
    "            for t in range(T-1):\n",
    "                # target sequence should shift by one time-step, because we are predicting the next word\n",
    "                loss = loss + F.nll_loss(F.log_softmax(pred[:, t]), tgt[:, t+1], ignore_index=0)\n",
    "                \n",
    "\n",
    "            return loss, pred.argmax(dim=-1)\n",
    "\n",
    "        else:\n",
    "            \"\"\"\n",
    "            When predicting a sequence given the 'bos' token, the input for the next step is the predicted\n",
    "            token from the previous time step.\n",
    "            \"\"\"\n",
    "            inputs = tgt[:, :1] \n",
    "            preds = tgt.new_zeros(tgt.shape) # initialized with zeros, it could be anything,  because the self-attention in Transformer only ‘sees’ words before itself (those have already been predicted)\n",
    "            preds[:, :1] = tgt[:, :1]  # copy ‘bos’ token to preds\n",
    "            for t in range(MAX_LEN+1): \n",
    "                pred, h = self.dec(preds, h) \n",
    "                pred_idx = pred.argmax(dim=-1)\n",
    "                preds[:, t+1:t+2] = pred_idx[:, t:t+1] # \n",
    "\n",
    "            return preds[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 / 11700\tLoss:\t53.973858\n",
      "pred:\t tensor([209,  91, 375,  85, 320, 299, 241, 378, 113, 276], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 14,  79,  68,  41, 320, 177,   3,  11,   2], device='cuda:0')\n",
      "\n",
      "iter 780 / 11700\tLoss:\t5.423491\n",
      "pred:\t tensor([ 38, 116, 202,   3,  11,   2,  11,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 38, 116, 202,   3,  11,   2,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 1560 / 11700\tLoss:\t2.194106\n",
      "pred:\t tensor([156, 157,   3,  11,   2,  11,  11,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([155, 157,   3,  11,   2,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 2340 / 11700\tLoss:\t1.432838\n",
      "pred:\t tensor([ 38, 316,  41, 174,  11,   2,  11,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 38, 316,  41, 174,  11,   2,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 3120 / 11700\tLoss:\t1.135168\n",
      "pred:\t tensor([92,  3,  5,  2,  5,  5,  5,  5,  5,  5], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([92,  3,  5,  2,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 3900 / 11700\tLoss:\t1.543563\n",
      "pred:\t tensor([ 6,  3,  3, 11,  2, 11,  3, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 6,  3,  3, 11,  2,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 4680 / 11700\tLoss:\t1.002790\n",
      "pred:\t tensor([48,  3,  3, 11,  2, 11, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([48,  3,  3, 11,  2,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 5460 / 11700\tLoss:\t1.147186\n",
      "pred:\t tensor([36, 74,  3, 11,  2, 11, 11, 11, 11, 11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([36, 72,  3, 11,  2,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 6240 / 11700\tLoss:\t1.178602\n",
      "pred:\t tensor([176,   9,   3,  24,   2, 237,   3,   3,   3,   3], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([176,  74,   3,  24,   2,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 7020 / 11700\tLoss:\t1.391534\n",
      "pred:\t tensor([ 15, 205,  19,  11,   2,   3,   3,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 15, 205,  19,  11,   2,   0,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 7800 / 11700\tLoss:\t1.179958\n",
      "pred:\t tensor([ 36,  40,   3,  11,   2,  11,  40, 300, 300, 300], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([38, 40,  3, 11,  2,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 8580 / 11700\tLoss:\t1.246563\n",
      "pred:\t tensor([ 14, 197, 108,   3,  11,   2,  11,  11,  11,  11], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 14, 197,  72,   3,  11,   2,   0,   0,   0], device='cuda:0')\n",
      "\n",
      "iter 9360 / 11700\tLoss:\t1.359418\n",
      "pred:\t tensor([3, 3, 5, 2, 3, 5, 5, 5, 3, 3], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([6, 3, 5, 2, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "\n",
      "iter 10140 / 11700\tLoss:\t1.068574\n",
      "pred:\t tensor([ 14,   3,   5,   2, 108,   3,   3,   3,   3,   3], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([14,  3,  5,  2,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "\n",
      "iter 10920 / 11700\tLoss:\t0.941206\n",
      "pred:\t tensor([176,  78,   3,  24,   2,   6,  78,  78,  78,  78], device='cuda:0')\n",
      "\n",
      "tgt:\t tensor([ 6, 78,  3, 24,  2,  0,  0,  0,  0], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TransformerEncoder(200, 24, 48, 24, 8, 2, 0.5, device)\n",
    "# # training \n",
    "batch_size = 32+32\n",
    "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
    "embedding_dim, hidden_size, num_heads, num_layers, dropout = 64+64+64, 128+128, 4, 2, 0.0\n",
    "\n",
    "lr = (1e-4)*5\n",
    "epoch = 50+50+50\n",
    "device = torch.device('cuda:0') # cuda:0 if you have gpu\n",
    "net = Transformer(vocab_eng.num_word, vocab_fra.num_word, embedding_dim, hidden_size, num_heads, num_layers, dropout, device)\n",
    "net = net.to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss_list = []\n",
    "print_interval = len(train_iter)*10\n",
    "total_iter = epoch * len(train_iter)\n",
    "for e in range(epoch):\n",
    "    net.train()\n",
    "    for i, train_data in enumerate(train_iter):\n",
    "        train_data = [ds.to(device) for ds in train_data]\n",
    "\n",
    "        loss, pred = net(*train_data)\n",
    "\n",
    "        loss_list.append(loss.mean().detach())\n",
    "        optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step = i + e * len(train_iter)\n",
    "        if step % print_interval == 0:\n",
    "            print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
    "            print('pred:\\t {}\\n'.format(pred[0].detach().to(device)))\n",
    "            print('tgt:\\t {}\\n'.format(train_data[2][0][1:].to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Report\n",
    "\n",
    "- (5 points) Generate predictions with Transformer. During predicting, the decoder only sees the 'bos' token and encodings from encoder.\n",
    "- (5 points) Plot the loss curve, report the prediction accuracy and print out several (~5) prediction samples together with corresponding targets. Bonus points will be assigned depending on the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save the loss curve figure in a file for the report\n",
    "plt.plot(np.arange(len(loss_list)), loss_list)\n",
    "plt.title('Loss Curve of transformer seq2sq model')\n",
    "plt.savefig('./report_fig/loss_curve_transformer.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:\t ['fais', 'ce', 'que', 'je', 'dis', '.', 'unk', '.']\n",
      "\n",
      "tgt:\t ['faites', 'ce', 'que', 'je', 'dis', '.', 'eos', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['je', 'ferai', 'ma', 'unk', '.', 'unk', '.']\n",
      "\n",
      "tgt:\t ['je', 'ferai', 'mon', 'unk', '.', 'eos', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['je', 'travaille', 'seul', '.', '.', '.']\n",
      "\n",
      "tgt:\t ['je', 'travaille', 'seule', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "pred:\t ['il', 'unk', 'des', 'unk', '.', '.', '.']\n",
      "\n",
      "tgt:\t ['il', 'a', 'unk', 'de', 'la', 'unk', '.', 'eos', 'pad']\n",
      "\n",
      "pred:\t ['il', 'a', 'unk', '.', '.', '.']\n",
      "\n",
      "tgt:\t ['il', 'se', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
      "\n",
      "Prediction Acc.: 0.6617\n"
     ]
    }
   ],
   "source": [
    "def comp_acc(pred, gt, valid_len, device):\n",
    "    N, T_gt = gt.shape[:2]\n",
    "    _, T_pr = pred.shape[:2]\n",
    "    assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
    "    len_mask = torch.arange(T_gt).expand(N, T_gt).to(device)\n",
    "    len_mask = (len_mask < valid_len[:, None])\n",
    "\n",
    "    pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
    "    pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
    "    return pred_acc\n",
    "  \n",
    "net.eval()# for predicting, we need turn off the net.training by calling eval()\n",
    "acc_list = []\n",
    "for i, train_data in enumerate(train_iter):\n",
    "    train_data = [ds.to(device) for ds in train_data]\n",
    "    pred = net(*train_data)\n",
    "    pred_acc = comp_acc(pred.detach().to(device), train_data[2].detach().to(device)[:, 1:], train_data[3].to(device), device)\n",
    "    acc_list.append(pred_acc)\n",
    "    if i < 5:# print 5 samples from 5 batches\n",
    "        pred = pred[0].detach().to(device)\n",
    "        pred_seq = []\n",
    "        for t in range(MAX_LEN+1):\n",
    "            pred_wd = vocab_fra.index2word[pred[t].item()] \n",
    "            if pred_wd != 'eos':\n",
    "                pred_seq.append(pred_wd)\n",
    "\n",
    "        print('pred:\\t {}\\n'.format(pred_seq))\n",
    "        print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].to(device)]))\n",
    "    \n",
    "print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
